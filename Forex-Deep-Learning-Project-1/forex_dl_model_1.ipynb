{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, CuDNNLSTM, BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ModelCheckpoint\n",
    "import time\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data format specified for data gotten from https://www.dukascopy.com/swiss/english/marketwatch/historical/\n",
    "class Dukascopy_Historical_Data:\n",
    "    \n",
    "    def __init__(self,folder_path):\n",
    "        self.folder_path = folder_path                                                                                 #name of the folder containing forex timeseries data     \n",
    "        self.filenames = [name for name in listdir(folder_path)]                                                       #extract all the file names in the folder\n",
    "        self.tickers = [filename.split('_')[0] for filename in self.filenames]                                         #extract ticker name from the file names\n",
    "        self.candlestick = self.filenames[0].split('_')[2] + ' ' + self.filenames[0].split('_')[3]                     #extract candlestick size from the filenames (assumption: candlestick size is the same for all the files\n",
    "        self.bid_ask = self.filenames[0].split('_')[4]                                                                 #are these bid prices or ask prices contained in the data?\n",
    "        self.time_frame = self.filenames[0].split('_')[5].split('.')[0]                                                #start time and end time                                                                                           \n",
    "        \n",
    "    def pd(self):\n",
    "        dict_ = {}                                                                                                     #create a dictionary such that {ticker: pd.DataFrame(OCHL data)} for each ticker in self.tickers\n",
    "        for ticker,filename in zip(self.tickers,self.filenames):\n",
    "            dict_[ticker] = pd.read_csv(self.folder_path+f'\\\\{filename}')\n",
    "            dict_[ticker].set_index(['Gmt time'],inplace=True)\n",
    "            dict_[ticker].set_index(pd.to_datetime(dict_[ticker].index,\n",
    "                                                   infer_datetime_format=True),inplace=True)\n",
    "            dict_[ticker].columns = [ticker+' '+str(col) for col in dict_[ticker].columns]\n",
    "        return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Currency_Pair:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 ticker,\n",
    "                 candlestick,\n",
    "                 dataset):\n",
    "                                                                                                                        #dataset assumed to be Dukascopy_Historical_Data object\n",
    "        self.ticker = ticker                                                                        \n",
    "        self.candlestick = candlestick                                                                                  #candlestick as fraction of 1 hour. eg 15min candlestick is 1/4\n",
    "        self.data = dataset.pd()[self.ticker]                                                                           #data in pd.DataFrame()\n",
    "        \n",
    "    #short_long = -1 {short trade}, short_long = 1 {long trade}    \n",
    "    def return_in_t(self,trade_type,start_date,end_date,):                                                              #trade simulation. given long or short opened at \"start_date\" compute return at \"close_date\"\n",
    "        short_long = {'Short':-1,'Long':1}                                                                              #trade type coefficient\n",
    "        output = ((self.data[end_date]['Close'][0]-self.data[start_date]['Close'][0])*short_long[trade_type],           #trades are entered end exited at \"Close\" price. return tuple s.t (net profit (commision excluded),\n",
    "                  (self.data[end_date]['Close'][0]/self.data[start_date]['Close'][0] - 1)*short_long[trade_type])       #                                                                  gain percentage))\n",
    "        return output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Matrix:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 ochl='Close'):\n",
    "        \n",
    "        self.data = dataset                                                                                             #assumed to be Dukascopy_Historical_Data object\n",
    "        self.ochl = ochl                                                                                                #Open,High,Close,Low to be the price taken into account in computations\n",
    "        \n",
    "    def pearson_corr_matrix(self):                                                                                      #for each ticker give tuple s.t (cross-currency correlation coefficient, \n",
    "        output = pd.DataFrame(columns=self.data.tickers,                                                                #                           p-value for h0 -> data follows normal distribution)\n",
    "                              index=self.data.tickers)                                                                  #put tuples of each ticker pair in n*n matrix \n",
    "        for base in self.data.tickers:\n",
    "            for quote in self.data.tickers:\n",
    "                if base==quote:\n",
    "                    output[base][quote] = 1\n",
    "                else:\n",
    "                    output[base][quote] = pearsonr(self.data.pd()[base][self.ochl],\n",
    "                                                   self.data.pd()[quote][self.ochl])\n",
    "        return output\n",
    "    \n",
    "    def normality_test(self,log=True):                                                                                  #test if data follows normal distribution return outcome and p value s.t h0 >> data is normal\n",
    "        output = pd.DataFrame(columns=self.data.tickers,\n",
    "                              index=self.data.tickers)\n",
    "    \n",
    "    def close(self,):                                                                                                                                                           \n",
    "        tickers = [ticker for ticker in self.data.tickers]                                                             #ticker names. ie access the dataset dictionary\n",
    "        close_vals = [self.data.pd()[ticker][self.ochl] for ticker in tickers]                                         #data taken form specified columns (ochl) for each ticker\n",
    "        output = pd.DataFrame(dict(zip(tickers,close_vals)))                                                           #combined dataframe\n",
    "        return output                                                                                                  #return pd.DataFrame \n",
    "    \n",
    "    def volume_close(self):                                                                                            \n",
    "        tickers = [ticker for ticker in self.data.tickers]                                                             #tickers\n",
    "        volumes = [self.data.pd()[ticker]['Volume'] for ticker in tickers]                                             #volumes                   \n",
    "        closes = [self.data.pd()[ticker]['Close'] for ticker in tickers]                                               #close prices\n",
    "        output = pd.DataFrame()                                                                                        #output dataframe\n",
    "        for i in range(len(tickers)):                                                                                  #for i smaller than size of ticker set...\n",
    "            output[tickers[i] + ' Close'] = closes[i]                                                                  #assign close values for ticker number i\n",
    "            output[tickers[i] + ' Volume'] = volumes[i]                                                                #assign volume values for ticker number i\n",
    "        return output\n",
    "    \n",
    "    def volume_close_ma(self,ma_len):                                                                                  #same as above but with volume,close and moving average of len ma_len\n",
    "        tickers = [ticker for ticker in self.data.tickers]\n",
    "        volumes = [self.data.pd()[ticker][ticker+' Volume'] for ticker in tickers]\n",
    "        closes = [self.data.pd()[ticker][ticker+' Close'] for ticker in tickers]\n",
    "        MAs = [self.data.pd()[ticker][ticker+' Close'].rolling(ma_len).mean() for ticker in tickers]\n",
    "        output = pd.DataFrame()\n",
    "        for i in range(len(tickers)):\n",
    "            output[tickers[i] + ' Close'] = closes[i]\n",
    "            output[tickers[i] + ' Volume'] = volumes[i]\n",
    "            output[tickers[i] + ' MA'] = MAs[i]\n",
    "        return output\n",
    "        \n",
    "    #+def strength_rating(self):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:                                                                                                         #collective class for variety of models                                                                                         \n",
    "\n",
    "    class RNN:\n",
    "        \n",
    "        def __init__(self,                                                     \n",
    "                     data,                                                                                           #pd.DataFram() as data\n",
    "                     candlestick,                                                                                    #candlestick type (1m,15m,1h,...)  \n",
    "                     y_ratio,                                                                                        #price growth/drop of specific ticker model will predict\n",
    "                     period_to_predict,                                                                              #how many candles ahead is the model predicting growth/drop\n",
    "                     sequence_length,                                                                                #on how many data points is the predictoin based\n",
    "                     validation_size,                                                                                #% of the data set used as the validation set\n",
    "                     scale_range,                                                                                    #all data scaled within this range\n",
    "                     epochs,                                                                                         #how many epochs per model\n",
    "                     batch_size):                                                                                    #batch size ie data input size for each epoch\n",
    "            \n",
    "            self.candlestick = candlestick\n",
    "            self.data = data\n",
    "            self.y_ratio = y_ratio\n",
    "            self.period_to_predict = period_to_predict\n",
    "            self.sequence_length = sequence_length\n",
    "            self.validation_size = validation_size\n",
    "            self.scale_range = scale_range\n",
    "            self.epochs = epochs\n",
    "            self.batch_size = batch_size\n",
    "            self.name = f'{self.sequence_length} {self.candlestick}-SEQ-{self.period_to_predict}-PRED-{int(time.time())}'       #name used to save model and weights (for later comparison)\n",
    "            \n",
    "        def classify(self,current,future):                                                                          #classify if the y_ticker data was greater(1) or smaller (0) period_to_predict steps ahead of current val\n",
    "            return (future>current).astype(int)                    \n",
    "        \n",
    "        def format_data(self):                                                                                      #format the data\n",
    "            X = self.data                                                                                           #predictors\n",
    "            y = self.data[self.y_ratio].shift(-self.period_to_predict)                                              #value to be predicted (y_ticker value shifted periods_to_predict steps \"from the future\")\n",
    "            main_df = X                                                                                             #create pd.DataFram()\n",
    "            main_df[f'future {self.y_ratio}'] = y                                                                   #create 'future price' column  of values of y                                     \n",
    "            main_df['target'] = self.classify(main_df[self.y_ratio],                                                #compare present value with future value using classify function. Add result as a calumn\n",
    "                                              main_df[f'future {self.y_ratio}'])\n",
    "            validation_df = main_df[int((1-self.validation_size)*len(main_df)):]                                    #split the formated data to training set and validation set\n",
    "            train_df = main_df[~main_df.isin(validation_df)].dropna()\n",
    "            return (train_df,validation_df)                                                                         #return tuple s.t (train set, validation set)\n",
    "        \n",
    "        def preprocess_df(self,df):\n",
    "            df = df[df[f'{self.y_ratio.split()[0]} Volume']!=0]                                                                        #every trading day (exclude weekends ie days with 0 volume)\n",
    "            df = df.drop(f'future {self.y_ratio}',axis=1)                                                                              #get rid of the future data to avoid look-ahead bias\n",
    "            df = df.dropna()                                                                                                           #drop NaN values\n",
    "            for col in df.columns:                                                                                                     #for each column...                                                                                    \n",
    "                if col != 'target':                                                                                                    #except for \"target\"...\n",
    "                    df[col]=df[col].pct_change()                                                                                       #change values from absolute values to relative % change                                                                                                 \n",
    "                    df.dropna(inplace=True)                                                                                            #drop na values caused by the above transformation\n",
    "                    df[col] = MinMaxScaler(feature_range=self.scale_range).fit_transform(df[col].values.reshape(-1,1))                 #scale all the values within the column to be in range self.scale_range\n",
    "                    #df[col] = preprocessing.scale(df[col].values)\n",
    "            df.dropna(inplace=True)                                                                                                    #drop the na values again\n",
    "              \n",
    "            sequential_data = []                                                                                    #a set of price sequences of length self.sequence_length \n",
    "            prev_candles = deque(maxlen=self.sequence_length)                                                       #creates each of the sequences \n",
    "            for row in df.values:                                                                                   #for each row of data in df...\n",
    "                prev_candles.append([value for value in row[:-1]])                                                  #add the row to deque (except for target)\n",
    "                if len(prev_candles)==self.sequence_length:                                                         #if the size of the current sequence is of length defined by self.sequence length...\n",
    "                    sequential_data.append([np.array(prev_candles),row[-1]])                                        #add the entire sequence and the target to the sequantial_data set ie [data,target]\n",
    "            random.shuffle(sequential_data)                                                                         #mix the order of the sequences inside the set\n",
    "            \n",
    "            buys = []                                                                                               #number of optimal long trades\n",
    "            sells = []                                                                                              #number of optimal short trades\n",
    "            for seq,target in sequential_data:                                                                      #for sequence and target (1 or 0) in sequantial data...\n",
    "                if target==0:                                                                                       #if target = 0...\n",
    "                    sells.append([seq,target])                                                                      #add the sequence,target to sells as sequence that should 'predict' to short\n",
    "                else: \n",
    "                    buys.append([seq,target])                                                                       #otherwise add the sequence to buys as sequence that 'predict' to buy\n",
    "            random.shuffle(buys)                                                                                    #shuffle the order of the sequences in buys \n",
    "            random.shuffle(sells)                                                                                   #shuffle the order of the sequences in sells\n",
    "            lower = min(len(buys),len(sells))                                                                       #the size of smaller set ie either size of buys or size of sells\n",
    "            buys = buys[:lower]                                                                                     #make the buy and sell sets sizes equal to lower (do this to prevent model from setting target to the value\n",
    "            sells = sells[:lower]                                                                                   #more often occuring in the dataset ie \"60% of the sequences in the data has target 1 ---> always predict long\")                                                                        \n",
    "                                                                                                                      \n",
    "            sequential_data = buys+sells                                                                            #join the buys and sells sets\n",
    "            random.shuffle(sequential_data)                                                                         #shuffle\n",
    "            X = []                                                                                                  #predictors (data)\n",
    "            y = []                                                                                                  #target\n",
    "            for seq, target in sequential_data:                                                                     #for sequence and target in sequential_data...\n",
    "                X.append(seq)                                                                                       #add sequence to X(predictors)                                                                \n",
    "                y.append(target)                                                                                    #add target to y(target)\n",
    "                \n",
    "            return np.array(X),y                                                                                    #return tuple of  numpy array of X and y\n",
    "        \n",
    "        def train_set(self):\n",
    "            return self.preprocess_df(self.format_data()[0])                                                        #create a training set\n",
    "            \n",
    "        def validation_set(self):\n",
    "            return self.preprocess_df(self.format_data()[1])                                                        #create a validation set\n",
    "        \n",
    "        def run(self):\n",
    "            model = Sequential()                                                                                    #initialize sequential model\n",
    "            model.add(LSTM(128,input_shape=(self.train_set()[0].shape[1:]),return_sequences=True))                  #add first layer for the train_set \n",
    "            model.add(Dropout(0.2))                                                                                 #dropout to prevent overfitting \n",
    "            model.add(BatchNormalization())\n",
    "                      \n",
    "            model.add(LSTM(128,input_shape=(self.train_set()[0].shape[1:]),return_sequences=True))                 #add second layer\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(BatchNormalization())\n",
    "                      \n",
    "            model.add(LSTM(128,input_shape=(self.train_set()[0].shape[1:])))                                       #add third layer\n",
    "            model.add(Dropout(0.2))\n",
    "            model.add(BatchNormalization())\n",
    "                      \n",
    "            model.add(Dense(32,activation='tanh'))                                                                 #add activation layer \n",
    "            model.add(Dropout(0.2))\n",
    "            \n",
    "            model.add(Dense(2,activation='softmax'))                                                               #add activation layer\n",
    "                      \n",
    "            opt = tf.keras.optimizers.Adam(lr=0.001,decay=1e-6)                                                    #model optimezer\n",
    "            \n",
    "            model.compile(loss='sparse_categorical_crossentropy',                                                  #compile the model given the above parameters\n",
    "                          optimizer=opt,\n",
    "                          metrics=['accuracy'])\n",
    "            \n",
    "            history = model.fit(self.train_set()[0],self.train_set()[1],\n",
    "                                batch_size=self.batch_size,\n",
    "                                epochs=self.epochs,\n",
    "                                validation_data=(self.validation_set()[0],self.validation_set()[1]))               #fit the train_data, validation_data into the compiled model\n",
    "            \n",
    "            return history                                                                                         #return fitted model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1014 15:33:18.542689 17296 deprecation.py:506] From C:\\Users\\mateu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 630 samples, validate on 94 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1014 15:33:19.920172 17296 deprecation.py:323] From C:\\Users\\mateu\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "630/630 [==============================] - 4s 7ms/sample - loss: 0.8406 - acc: 0.5159 - val_loss: 0.6933 - val_acc: 0.4787\n",
      "Epoch 2/10\n",
      "630/630 [==============================] - 2s 4ms/sample - loss: 0.8398 - acc: 0.4825 - val_loss: 0.6934 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      "630/630 [==============================] - 3s 4ms/sample - loss: 0.8129 - acc: 0.5175 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      "630/630 [==============================] - 3s 4ms/sample - loss: 0.8061 - acc: 0.4921 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      "630/630 [==============================] - 3s 5ms/sample - loss: 0.8142 - acc: 0.5175 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      "630/630 [==============================] - 3s 5ms/sample - loss: 0.8034 - acc: 0.4968 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "630/630 [==============================] - 3s 5ms/sample - loss: 0.7661 - acc: 0.5429 - val_loss: 0.6936 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "630/630 [==============================] - 3s 5ms/sample - loss: 0.7797 - acc: 0.5317 - val_loss: 0.6939 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "630/630 [==============================] - 3s 5ms/sample - loss: 0.7444 - acc: 0.5413 - val_loss: 0.6941 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "630/630 [==============================] - 3s 5ms/sample - loss: 0.7597 - acc: 0.5429 - val_loss: 0.6942 - val_acc: 0.5000\n",
      "Train on 596 samples, validate on 76 samples\n",
      "Epoch 1/10\n",
      "596/596 [==============================] - 5s 9ms/sample - loss: 0.9209 - acc: 0.5101 - val_loss: 0.6929 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      "596/596 [==============================] - 3s 5ms/sample - loss: 0.8594 - acc: 0.5101 - val_loss: 0.6925 - val_acc: 0.5395\n",
      "Epoch 3/10\n",
      "596/596 [==============================] - 3s 5ms/sample - loss: 0.9030 - acc: 0.5000 - val_loss: 0.6921 - val_acc: 0.5526\n",
      "Epoch 4/10\n",
      "596/596 [==============================] - 3s 5ms/sample - loss: 0.8112 - acc: 0.5252 - val_loss: 0.6919 - val_acc: 0.5789\n",
      "Epoch 5/10\n",
      "596/596 [==============================] - 3s 5ms/sample - loss: 0.8720 - acc: 0.4899 - val_loss: 0.6921 - val_acc: 0.5526\n",
      "Epoch 6/10\n",
      "596/596 [==============================] - 3s 5ms/sample - loss: 0.8571 - acc: 0.5201 - val_loss: 0.6926 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      "596/596 [==============================] - 3s 5ms/sample - loss: 0.8470 - acc: 0.5034 - val_loss: 0.6928 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      "596/596 [==============================] - 3s 5ms/sample - loss: 0.8158 - acc: 0.5218 - val_loss: 0.6925 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      "596/596 [==============================] - 3s 6ms/sample - loss: 0.8433 - acc: 0.5168 - val_loss: 0.6922 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      "596/596 [==============================] - 3s 6ms/sample - loss: 0.8418 - acc: 0.4883 - val_loss: 0.6925 - val_acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "if __name__ =='__main__':\n",
    "    \n",
    "    data = Dukascopy_Historical_Data('1h_majors')                    #create data objet\n",
    "    df_multi = Data_Matrix(data).volume_close_ma(20)                 #create dataframe for multiple tickers\n",
    "    gbpusd = data.pd()['GBPUSD']                                     #create dataframe for one given ticker\n",
    "    \n",
    "    multi_df_model = Model.RNN(data=df_multi,                       #model fitting multiple currency pairs as the predictors\n",
    "                 candlestick='1h',\n",
    "                 y_ratio='GBPUSD Close',\n",
    "                 period_to_predict=1,\n",
    "                 sequence_length=72,\n",
    "                 validation_size=0.2,\n",
    "                 scale_range=(0,1),\n",
    "                 epochs=10,\n",
    "                 batch_size=512)\n",
    "    \n",
    "    single_df_model = Model.RNN(data=gbpusd,                      #model fitting just single currency pair as the predictor\n",
    "                 candlestick='1h',\n",
    "                 y_ratio='GBPUSD Close',\n",
    "                 period_to_predict=1,\n",
    "                 sequence_length=72,\n",
    "                 validation_size=0.2,\n",
    "                 scale_range=(0,1),\n",
    "                 epochs=10,\n",
    "                 batch_size=512)\n",
    "    \n",
    "    single_df_model.run()                                     #run both the models\n",
    "    multi_df_model.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 630 samples, validate on 94 samples\n",
      "Epoch 1/20\n",
      "630/630 [==============================] - 6s 10ms/sample - loss: 0.9081 - acc: 0.5000 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 2/20\n",
      "630/630 [==============================] - 4s 7ms/sample - loss: 0.9453 - acc: 0.4762 - val_loss: 0.6934 - val_acc: 0.5000\n",
      "Epoch 3/20\n",
      "630/630 [==============================] - 4s 7ms/sample - loss: 0.8371 - acc: 0.5159 - val_loss: 0.6935 - val_acc: 0.5000\n",
      "Epoch 4/20\n",
      "630/630 [==============================] - 4s 7ms/sample - loss: 0.8295 - acc: 0.5190 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Epoch 5/20\n",
      "630/630 [==============================] - 4s 7ms/sample - loss: 0.8148 - acc: 0.5429 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 6/20\n",
      "630/630 [==============================] - 4s 7ms/sample - loss: 0.8644 - acc: 0.4857 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 7/20\n",
      "630/630 [==============================] - 4s 7ms/sample - loss: 0.8365 - acc: 0.5302 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 8/20\n",
      "630/630 [==============================] - 4s 7ms/sample - loss: 0.8309 - acc: 0.5302 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 9/20\n",
      "630/630 [==============================] - 5s 7ms/sample - loss: 0.7754 - acc: 0.5381 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 10/20\n",
      "630/630 [==============================] - 5s 7ms/sample - loss: 0.8040 - acc: 0.5079 - val_loss: 0.6931 - val_acc: 0.4787\n",
      "Epoch 11/20\n",
      "630/630 [==============================] - 5s 7ms/sample - loss: 0.7998 - acc: 0.4921 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 12/20\n",
      "630/630 [==============================] - 5s 7ms/sample - loss: 0.7854 - acc: 0.4984 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 13/20\n",
      "630/630 [==============================] - 5s 7ms/sample - loss: 0.8108 - acc: 0.4762 - val_loss: 0.6931 - val_acc: 0.5000\n",
      "Epoch 14/20\n",
      "630/630 [==============================] - 5s 7ms/sample - loss: 0.8091 - acc: 0.5159 - val_loss: 0.6932 - val_acc: 0.5000\n",
      "Epoch 15/20\n",
      "630/630 [==============================] - 5s 7ms/sample - loss: 0.7685 - acc: 0.5270 - val_loss: 0.6934 - val_acc: 0.5000\n",
      "Epoch 16/20\n",
      "630/630 [==============================] - 5s 8ms/sample - loss: 0.8273 - acc: 0.4873 - val_loss: 0.6937 - val_acc: 0.5000\n",
      "Epoch 17/20\n",
      "630/630 [==============================] - 5s 7ms/sample - loss: 0.7918 - acc: 0.5016 - val_loss: 0.6938 - val_acc: 0.5000\n",
      "Epoch 18/20\n",
      "630/630 [==============================] - 5s 7ms/sample - loss: 0.7658 - acc: 0.5190 - val_loss: 0.6939 - val_acc: 0.5000\n",
      "Epoch 19/20\n",
      "630/630 [==============================] - 5s 7ms/sample - loss: 0.7856 - acc: 0.5016 - val_loss: 0.6941 - val_acc: 0.5000\n",
      "Epoch 20/20\n",
      "630/630 [==============================] - 5s 8ms/sample - loss: 0.7632 - acc: 0.5333 - val_loss: 0.6942 - val_acc: 0.5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d7ec0f6d30>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
